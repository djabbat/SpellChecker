#!/usr/bin/env python3
"""
–ì—Ä—É–∑–∏–Ω—Å–∫–∏–π —Å–ø–µ–ª–ª—á–µ–∫–µ—Ä - –ø–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
–í–∫–ª—é—á–∞–µ—Ç —Å–±–æ—Ä–∫—É –∫–æ—Ä–ø—É—Å–∞, –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ–≤–µ—Ä–∫—É –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏–∏
"""

import re
import os
import json
from collections import Counter, defaultdict
from pathlib import Path
import pickle
import argparse
from typing import List, Tuple, Set
import itertools

def levenshtein_distance(s1: str, s2: str) -> int:
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ –º–µ–∂–¥—É –¥–≤—É–º—è —Å—Ç—Ä–æ–∫–∞–º–∏"""
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)
    
    if len(s2) == 0:
        return len(s1)
    
    previous_row = list(range(len(s2) + 1))
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    
    return previous_row[-1]

class GeorgianSpellChecker:
    def __init__(self):
        self.vocabulary = set()
        self.word_freq = Counter()
        self.ngram_models = {}
        
    def load_corpus(self, corpus_path: str) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ—Ä–ø—É—Å–∞ –∏–∑ –ø–∞–ø–∫–∏"""
        corpus_dir = Path(corpus_path)
        
        if not corpus_dir.exists():
            # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∫–æ—Ä–ø—É—Å –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            parent_corpus = Path("..") / corpus_path
            if parent_corpus.exists():
                corpus_dir = parent_corpus
                print(f"–ö–æ—Ä–ø—É—Å –Ω–∞–π–¥–µ–Ω –≤: {parent_corpus}")
            else:
                # –ü–æ–ø—Ä–æ–±—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –æ—Ç –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
                project_root = Path(__file__).parent.parent
                absolute_corpus = project_root / corpus_path
                if absolute_corpus.exists():
                    corpus_dir = absolute_corpus
                    print(f"–ö–æ—Ä–ø—É—Å –Ω–∞–π–¥–µ–Ω –≤: {absolute_corpus}")
                else:
                    raise FileNotFoundError(f"–ü–∞–ø–∫–∞ –∫–æ—Ä–ø—É—Å–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {corpus_path}. –ò—Å–∫–∞–ª–∏ –≤: {corpus_path}, {parent_corpus}, {absolute_corpus}")
        
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ—Ä–ø—É—Å–∞...")
        total_files = 0
        total_words = 0
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ txt —Ñ–∞–π–ª—ã –≤ –∫–æ—Ä–ø—É—Å–µ - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ß–ê–°–¢–¨
        txt_files = list(corpus_dir.rglob("*.txt"))  # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
        
        if not txt_files:
            print(f"–í –ø–∞–ø–∫–µ {corpus_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ txt —Ñ–∞–π–ª–æ–≤!")
            return
            
        for file_path in txt_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
                words = self.tokenize_georgian(content)
                if words:  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Å–ª–æ–≤–∞
                    self.vocabulary.update(words)
                    self.word_freq.update(words)
                
                total_files += 1
                total_words += len(words)
                
                if total_files % 100 == 0:
                    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {total_files}, —Å–ª–æ–≤: {total_words}")
                    
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_path}: {e}")
        
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –§–∞–π–ª–æ–≤: {total_files}, –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(self.vocabulary)}")
    
    def tokenize_georgian(self, text: str) -> List[str]:
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≥—Ä—É–∑–∏–Ω—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'[^\u10A0-\u10FF\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        words = text.strip().split()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
        return [word for word in words if len(word) > 1]
    
    def build_ngram_model(self, n: int = 2) -> None:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ N-gram –º–æ–¥–µ–ª–∏"""
        print(f"–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ {n}-gram –º–æ–¥–µ–ª–∏...")
        
        # –î–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω—É–∂–µ–Ω –¥–æ—Å—Ç—É–ø –∫ –∏—Å—Ö–æ–¥–Ω—ã–º —Ç–µ–∫—Å—Ç–∞–º
        ngram_model = defaultdict(Counter)
        
        # –ó–¥–µ—Å—å –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∫–æ—Ä–ø—É—Å –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è N-gram
        # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ —Å –∏—Ö —á–∞—Å—Ç–æ—Ç–∞–º–∏
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤
        
        self.ngram_models[n] = ngram_model
        print(f"{n}-gram –º–æ–¥–µ–ª—å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞")
    
    def train_from_cleaned_corpus(self, cleaned_corpus_path: str) -> None:
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —É–∂–µ –æ—á–∏—â–µ–Ω–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ"""
        cleaned_dir = Path(cleaned_corpus_path)
        
        if not cleaned_dir.exists():
            # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            parent_cleaned = Path("..") / cleaned_corpus_path
            if parent_cleaned.exists():
                cleaned_dir = parent_cleaned
                print(f"–û—á–∏—â–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å –Ω–∞–π–¥–µ–Ω –≤: {parent_cleaned}")
            else:
                print(f"–û—á–∏—â–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å –Ω–µ –Ω–∞–π–¥–µ–Ω: {cleaned_corpus_path}")
                return
        
        print("–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ—á–∏—â–µ–Ω–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ...")
        
        for file_type in ['cleaned', 'tokenized']:
            corpus_path = cleaned_dir / file_type
            if corpus_path.exists():
                txt_files = list(corpus_path.glob("*.txt"))
                if txt_files:
                    print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(txt_files)} —Ñ–∞–π–ª–æ–≤ –∏–∑ {file_type}...")
                    for file_path in txt_files:
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                content = f.read()
                            
                            words = content.split()
                            if words:  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Å–ª–æ–≤–∞
                                self.vocabulary.update(words)
                                self.word_freq.update(words)
                            
                        except Exception as e:
                            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {file_path}: {e}")
        
        print(f"–°–ª–æ–≤–∞—Ä—å –æ–±–Ω–æ–≤–ª–µ–Ω. –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(self.vocabulary)}")
    
    def is_correct(self, word: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –µ—Å—Ç—å –ª–∏ —Å–ª–æ–≤–æ –≤ —Å–ª–æ–≤–∞—Ä–µ"""
        return word in self.vocabulary
    
    def generate_candidates(self, word: str, max_distance: int = 2) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è"""
        candidates = []
        
        # –ï—Å–ª–∏ —Å–ª–æ–≤–æ —É–∂–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ
        if self.is_correct(word):
            return [word]
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞
        for candidate in self.vocabulary:
            distance = levenshtein_distance(word, candidate)
            if distance <= max_distance:
                candidates.append((candidate, distance))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é –∏ —á–∞—Å—Ç–æ—Ç–µ
        candidates.sort(key=lambda x: (x[1], -self.word_freq.get(x[0], 0)))
        
        return [candidate for candidate, distance in candidates[:10]]
    
    def suggest_corrections(self, word: str, max_suggestions: int = 5) -> List[str]:
        """–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –¥–ª—è —Å–ª–æ–≤–∞"""
        candidates = self.generate_candidates(word)
        return candidates[:max_suggestions]
    
    def check_text(self, text: str) -> List[Tuple[str, List[str]]]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π"""
        errors = []
        words = self.tokenize_georgian(text)
        
        for word in words:
            if not self.is_correct(word):
                suggestions = self.suggest_corrections(word)
                errors.append((word, suggestions))
        
        return errors
    
    def save_model(self, model_path: str) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        model_data = {
            'vocabulary': list(self.vocabulary),
            'word_freq': dict(self.word_freq),
            'ngram_models': self.ngram_models
        }
        
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
    
    def load_model(self, model_path: str) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        with open(model_path, 'rb') as f:
            model_data = pickle.load(f)
        
        self.vocabulary = set(model_data['vocabulary'])
        self.word_freq = Counter(model_data['word_freq'])
        self.ngram_models = model_data['ngram_models']
        
        print(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(self.vocabulary)}")

class CorpusProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ—Ä–ø—É—Å–∞"""
    
    @staticmethod
    def process_existing_corpus(corpus_path: str, output_path: str) -> None:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∫–æ—Ä–ø—É—Å–∞"""
        print("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ—Ä–ø—É—Å–∞ –¥–ª—è —Å–ø–µ–ª–ª—á–µ–∫–µ—Ä–∞...")
        
        corpus_dir = Path(corpus_path)
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø—É—Ç–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ—Ä–ø—É—Å–∞
        if not corpus_dir.exists():
            parent_corpus = Path("..") / corpus_path
            if parent_corpus.exists():
                corpus_dir = parent_corpus
                print(f"–ö–æ—Ä–ø—É—Å –Ω–∞–π–¥–µ–Ω –≤: {parent_corpus}")
            else:
                project_root = Path(__file__).parent.parent
                absolute_corpus = project_root / corpus_path
                if absolute_corpus.exists():
                    corpus_dir = absolute_corpus
                    print(f"–ö–æ—Ä–ø—É—Å –Ω–∞–π–¥–µ–Ω –≤: {absolute_corpus}")
                else:
                    print(f"–ö–æ—Ä–ø—É—Å –Ω–µ –Ω–∞–π–¥–µ–Ω: {corpus_path}")
                    return
        
        output_dir = Path(output_path)
        output_dir.mkdir(exist_ok=True)
        
        all_words = Counter()
        total_files = 0
        
        txt_files = list(corpus_dir.rglob("*.txt"))  # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
        if not txt_files:
            print(f"–í –ø–∞–ø–∫–µ {corpus_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ txt —Ñ–∞–π–ª–æ–≤!")
            return
            
        for file_path in txt_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
                words = re.findall(r'[\u10A0-\u10FF]{2,}', content)
                if words:  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Å–ª–æ–≤–∞
                    all_words.update(words)
                
                total_files += 1
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_path}: {e}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–∞—Ä—å
        vocabulary_file = output_dir / "vocabulary.txt"
        with open(vocabulary_file, 'w', encoding='utf-8') as f:
            for word, count in all_words.most_common():
                f.write(f"{word}\t{count}\n")
        
        print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –§–∞–π–ª–æ–≤: {total_files}, –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(all_words)}")
        print(f"–°–ª–æ–≤–∞—Ä—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {vocabulary_file}")

def create_hunspell_files(vocabulary: Set[str], output_dir: str) -> None:
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –¥–ª—è Hunspell"""
    hunspell_dir = Path(output_dir)
    hunspell_dir.mkdir(parents=True, exist_ok=True)
    
    # .dic —Ñ–∞–π–ª (—Å–ª–æ–≤–∞—Ä—å)
    dic_file = hunspell_dir / "ka_GE.dic"
    with open(dic_file, 'w', encoding='utf-8') as f:
        f.write(f"{len(vocabulary)}\n")
        for word in sorted(vocabulary):
            f.write(f"{word}\n")
    
    # .aff —Ñ–∞–π–ª (–∞—Ñ—Ñ–∏–∫—Å—ã) - –±–∞–∑–æ–≤–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –≥—Ä—É–∑–∏–Ω—Å–∫–æ–≥–æ
    aff_file = hunspell_dir / "ka_GE.aff"
    aff_content = """SET UTF-8
TRY ·Éê·Éë·Éí·Éì·Éî·Éï·Éñ·Éó·Éò·Éô·Éö·Éõ·Éú·Éù·Éû·Éü·É†·É°·É¢·É£·É§·É•·É¶·Éß·É®·É©·É™·É´·É¨·É≠·ÉÆ·ÉØ·É∞
"""
    
    with open(aff_file, 'w', encoding='utf-8') as f:
        f.write(aff_content)
    
    print(f"Hunspell —Ñ–∞–π–ª—ã —Å–æ–∑–¥–∞–Ω—ã –≤: {hunspell_dir}")

def find_corpus_path(corpus_path: str) -> Path:
    """–ù–∞—Ö–æ–¥–∏—Ç –ø—É—Ç—å –∫ –∫–æ—Ä–ø—É—Å—É, –ø—Ä–æ–≤–µ—Ä—è—è —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã"""
    paths_to_try = [
        Path(corpus_path),
        Path("corpus"),  # ·Éî·É° ·Éò·Éû·Éù·Éï·Éò·É° ·Éó·É•·Éï·Éî·Éú·É° ·Éô·Éù·É†·Éû·É£·É°·É°!
        Path("../1_collect/corpus"),
        Path("1_collect/corpus"), 
        Path("..") / corpus_path,
        Path(__file__).parent.parent / corpus_path,
        Path(__file__).parent / corpus_path,
    ]
    
    for path in paths_to_try:
        if path.exists():
            print(f"–ö–æ—Ä–ø—É—Å –Ω–∞–π–¥–µ–Ω: {path}")
            # ·Éì·Éê·Éï·Éê·Éë·É†·É£·Éú·Éù·Éó ·Éê·Éë·É°·Éù·Éö·É£·É¢·É£·É†·Éò ·Éû·Éê·Éó·Éò
            return path.resolve()
    
    # ·Éó·É£ ·Éô·Éù·É†·Éû·É£·É°·Éò ·Éê·É† ·Éõ·Éù·Éò·É´·Éî·Éë·Éú·Éê, ·É®·Éî·Éï·É•·Éõ·Éú·Éê·Éó ·É¢·Éî·É°·É¢·É£·É†·Éò
    print("–ö–æ—Ä–ø—É—Å –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –∫–æ—Ä–ø—É—Å...")
    test_corpus = Path("test_corpus")
    test_corpus.mkdir(exist_ok=True)
    
    test_texts = [
        "·Éí·Éê·Éõ·Éê·É†·ÉØ·Éù·Éë·Éê ·É†·Éù·Éí·Éù·É† ·ÉÆ·Éê·É† ·Éì·É¶·Éî·É° ·Éô·Éê·É†·Éí·Éò ·Éê·Éõ·Éò·Éú·Éì·Éò·Éê",
        "·É°·Éê·É•·Éê·É†·Éó·Éï·Éî·Éö·Éù ·É•·Éê·É†·Éó·É£·Éö·Éò ·Éî·Éú·Éê ·Éû·É†·Éù·Éí·É†·Éê·Éõ·Éò·É†·Éî·Éë·Éê ·Éô·Éù·Éõ·Éû·Éò·É£·É¢·Éî·É†·Éò",
        "·Éó·Éë·Éò·Éö·Éò·É°·Éò ·É´·Éê·Éö·Éò·Éê·Éú ·Éö·Éê·Éõ·Éê·Éñ·Éò ·É•·Éê·Éö·Éê·É•·Éò·Éê ·Éì·Éê ·Éõ·Éù·É°·É¨·Éù·Éú·É° ·É¢·É£·É†·Éò·É°·É¢·Éî·Éë·É°",
        "·Éû·É†·Éù·Éí·É†·Éê·Éõ·Éê ·É¨·Éî·É†·É° ·Éô·Éù·Éì·É° ·Éû·Éò·Éó·Éù·Éú·Éò·É° ·Éî·Éú·Éê·Éñ·Éî ·Éì·Éê ·Éê·Éõ·Éù·É¨·Éõ·Éî·Éë·É° ·É¢·Éî·É•·É°·É¢·É°"
    ]
    
    for i, text in enumerate(test_texts, 1):
        with open(test_corpus / f"test_{i}.txt", 'w', encoding='utf-8') as f:
            f.write(text)
    
    print(f"–°–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π –∫–æ—Ä–ø—É—Å: {test_corpus}")
    return test_corpus

def build_complete_spellchecker():
    """–ü–æ–ª–Ω–∞—è —Å–±–æ—Ä–∫–∞ —Å–ø–µ–ª–ª—á–µ–∫–µ—Ä–∞ –∏–∑ –∫–æ—Ä–ø—É—Å–∞"""
    
    # ·É®·Éî·Éï·É™·Éï·Éê·Éö·Éù·Éó ·Éû·Éê·Éó·Éî·Éë·Éò
    CORPUS_PATH = "corpus"  # ·Éê·ÉÆ·Éö·Éê ·Éî·É° ·Éò·Éõ·É£·É®·Éê·Éï·Éî·Éë·É°!
    CLEANED_CORPUS_PATH = "cleaned_corpus"
    MODEL_PATH = "georgian_spellchecker.pkl"
    
    print("=== –ü–û–õ–ù–ê–Ø –°–ë–û–†–ö–ê –ì–†–£–ó–ò–ù–°–ö–û–ì–û –°–ü–ï–õ–õ–ß–ï–ö–ï–†–ê ===")
    
    # ·É®·Éî·Éï·É•·Éõ·Éú·Éê·Éó ·É°·Éû·Éî·Éö·É©·Éî·Éô·Éî·É†·Éò
    spell_checker = GeorgianSpellChecker()
    
    # ·Éï·Éò·Éû·Éù·Éï·Éù·Éó ·Éô·Éù·É†·Éû·É£·É°·Éò
    corpus_dir = find_corpus_path(CORPUS_PATH)
    
    print(f"1. –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ—Ä–ø—É—Å: {corpus_dir}")
    
    # ·Éï·Éê·Éõ·Éù·É¨·Éõ·Éî·Éë·Éó ·É§·Éê·Éò·Éö·Éî·Éë·É°
    txt_files = list(corpus_dir.rglob("*.txt"))  # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
    print(f"   –ù–∞–π–¥–µ–Ω–æ txt —Ñ–∞–π–ª–æ–≤: {len(txt_files)}")
    for f in txt_files[:5]:  # –ü–æ–∫–∞–∂–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 —Ñ–∞–π–ª–æ–≤
        print(f"   - {f.name} ({f.stat().st_size} –±–∞–π—Ç)")
    
    # ·Éï·É¢·Éï·Éò·É†·Éó·Éê·Éï·Éó ·Éô·Éù·É†·Éû·É£·É°·É°
    spell_checker.load_corpus(str(corpus_dir))
    
    if len(spell_checker.vocabulary) == 0:
        print("–í–ù–ò–ú–ê–ù–ò–ï: –°–ª–æ–≤–∞—Ä—å –ø—É—Å—Ç! –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–ª–æ–≤–∞...")
        basic_words = {
            '·Éí·Éê·Éõ·Éê·É†·ÉØ·Éù·Éë·Éê', '·É†·Éù·Éí·Éù·É†', '·ÉÆ·Éê·É†', '·Éì·É¶·Éî·É°', '·Éô·Éê·É†·Éí·Éò', '·Éê·Éõ·Éò·Éú·Éì·Éò', 
            '·É°·Éê·É•·Éê·É†·Éó·Éï·Éî·Éö·Éù', '·Éó·Éë·Éò·Éö·Éò·É°·Éò', '·Éî·Éú·Éê', '·Éû·É†·Éù·Éí·É†·Éê·Éõ·Éê', '·Éô·Éù·Éõ·Éû·Éò·É£·É¢·Éî·É†·Éò',
            '·É´·Éê·Éö·Éò·Éê·Éú', '·Éö·Éê·Éõ·Éê·Éñ·Éò', '·É•·Éê·Éö·Éê·É•·Éò', '·É¢·É£·É†·Éò·É°·É¢·Éò', '·É¨·Éî·É†·É°', '·Éô·Éù·Éì·Éò'
        }
        spell_checker.vocabulary.update(basic_words)
        for word in basic_words:
            spell_checker.word_freq[word] = 1
    
    print(f"2. –°–ª–æ–≤–∞—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç {len(spell_checker.vocabulary)} —Å–ª–æ–≤")
    
    # ·Éï·Éê·É®·Éî·Éú·Éî·Éë·Éó N-gram ·Éõ·Éù·Éì·Éî·Éö·Éî·Éë·É°
    print("3. –°—Ç—Ä–æ–∏–º —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏...")
    spell_checker.build_ngram_model(2)
    spell_checker.build_ngram_model(3)
    
    # ·Éï·Éò·Éú·Éê·ÉÆ·Éê·Éï·Éó ·Éõ·Éù·Éì·Éî·Éö·É°
    print("4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å...")
    spell_checker.save_model(MODEL_PATH)
    
    # ·Éï·É•·Éõ·Éú·Éò·Éó Hunspell ·É§·Éê·Éò·Éö·Éî·Éë·É°
    print("5. –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª—ã –¥–ª—è Hunspell...")
    try:
        create_hunspell_files(spell_checker.vocabulary, "hunspell_georgian")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ Hunspell —Ñ–∞–π–ª–æ–≤: {e}")
        print("–ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø—É—Ç—å...")
        hunspell_dir = Path("hunspell_output")
        hunspell_dir.mkdir(exist_ok=True)
        
        dic_file = hunspell_dir / "ka_GE.dic"
        with open(dic_file, 'w', encoding='utf-8') as f:
            f.write(f"{len(spell_checker.vocabulary)}\n")
            for word in sorted(spell_checker.vocabulary):
                f.write(f"{word}\n")
        
        aff_file = hunspell_dir / "ka_GE.aff"
        aff_content = "SET UTF-8\nTRY ·Éê·Éë·Éí·Éì·Éî·Éï·Éñ·Éó·Éò·Éô·Éö·Éõ·Éú·Éù·Éû·Éü·É†·É°·É¢·É£·É§·É•·É¶·Éß·É®·É©·É™·É´·É¨·É≠·ÉÆ·ÉØ·É∞\n"
        with open(aff_file, 'w', encoding='utf-8') as f:
            f.write(aff_content)
        
        print(f"Hunspell —Ñ–∞–π–ª—ã —Å–æ–∑–¥–∞–Ω—ã –≤: {hunspell_dir}")
    
    # ·Éï·É¢·Éî·É°·É¢·Éê·Éï·Éó ·Éõ·Éù·Éì·Éî·Éö·É°
    print("6. –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å...")
    test_words = ['·Éí·Éê·Éõ·Éê·É†·ÉØ·Éù·Éë·Éê', '·Éí·Éê·Éõ·Éê·É†·ÉØ·Éê·Éë·Éê', '·É†·Éù·Éí·Éù·É†', '·É†·Éù·Éí·Éù·Éó']
    print("\n–¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏:")
    for word in test_words:
        if spell_checker.is_correct(word):
            print(f"  ‚úì {word} - –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ")
        else:
            suggestions = spell_checker.suggest_corrections(word)
            print(f"  ‚úó {word} -> {suggestions[:3]}")
    
    print("\n=== –°–ë–û–†–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê ===")
    print(f"–ú–æ–¥–µ–ª—å: {MODEL_PATH}")
    print(f"–°–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ: {len(spell_checker.vocabulary)}")
    print("–§–∞–π–ª—ã Hunspell: hunspell_output/")

def quick_test():
    """–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç —Å–ø–µ–ª–ª—á–µ–∫–µ—Ä–∞"""
    print("=== –ë–´–°–¢–†–´–ô –¢–ï–°–¢ ===")
    
    checker = GeorgianSpellChecker()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –µ—Å—Ç—å
    model_path = "georgian_spellchecker.pkl"
    if Path(model_path).exists():
        checker.load_model(model_path)
        print(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –°–ª–æ–≤: {len(checker.vocabulary)}")
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        test_texts = [
            "·Éí·Éê·Éõ·Éê·É†·ÉØ·Éù·Éë·Éê ·É†·Éù·Éí·Éù·É† ·ÉÆ·Éê·É†",
            "·Éí·Éê·Éõ·Éê·É†·ÉØ·Éê·Éë·Éê ·É†·Éù·Éí·Éù·Éó ·ÉÆ·Éê·É†·Éó", 
            "·Éî·É° ·Éê·É†·Éò·É° ·É°·Éê·É¢·Éî·É°·É¢·Éù ·É¢·Éî·É•·É°·É¢·Éò"
        ]
        
        for text in test_texts:
            print(f"\n–ü—Ä–æ–≤–µ—Ä–∫–∞: '{text}'")
            errors = checker.check_text(text)
            if errors:
                for word, suggestions in errors:
                    print(f"  –û—à–∏–±–∫–∞: '{word}' -> {suggestions[:3]}")
            else:
                print("  ‚úì –û—à–∏–±–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    else:
        print("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–±–æ—Ä–∫—É.")

def demo():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Å–ø–µ–ª–ª—á–µ–∫–µ—Ä–∞"""
    print("=== –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ì–†–£–ó–ò–ù–°–ö–û–ì–û –°–ü–ï–õ–õ–ß–ï–ö–ï–†–ê ===")
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —Å–ø–µ–ª–ª—á–µ–∫–µ—Ä
    checker = GeorgianSpellChecker()
    
    # –¢–µ—Å—Ç–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –∫–æ—Ä–ø—É—Å–∞)
    test_vocabulary = {
        '·Éí·Éê·Éõ·Éê·É†·ÉØ·Éù·Éë·Éê', '·É†·Éù·Éí·Éù·É†', '·ÉÆ·Éê·É†', '·Éì·É¶·Éî·É°', '·Éô·Éê·É†·Éí·Éò', '·Éê·Éõ·Éò·Éú·Éì·Éò', 
        '·É°·Éê·É•·Éê·É†·Éó·Éï·Éî·Éö·Éù', '·Éó·Éë·Éò·Éö·Éò·É°·Éò', '·Éî·Éú·Éê', '·Éû·É†·Éù·Éí·É†·Éê·Éõ·Éê', '·Éô·Éù·Éõ·Éû·Éò·É£·É¢·Éî·É†·Éò'
    }
    
    checker.vocabulary = test_vocabulary
    checker.word_freq = Counter(test_vocabulary)
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    test_words = [
        '·Éí·Éê·Éõ·Éê·É†·ÉØ·Éù·Éë·Éê',  # –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ
        '·Éí·Éê·Éõ·Éê·É†·ÉØ·Éê·Éë·Éê',  # –æ—à–∏–±–∫–∞
        '·É†·Éù·Éí·Éù·É†',      # –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ  
        '·É†·Éù·Éí·Éù·Éó',      # –æ—à–∏–±–∫–∞
        '·Éô·Éù·Éõ·Éû·É£·É¢·Éî·É†·Éò',  # –æ—à–∏–±–∫–∞
    ]
    
    print("\n–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–ª–æ–≤:")
    for word in test_words:
        if checker.is_correct(word):
            print(f"‚úì {word}")
        else:
            suggestions = checker.suggest_corrections(word)
            print(f"‚úó {word} -> {suggestions}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    parser = argparse.ArgumentParser(description='–ì—Ä—É–∑–∏–Ω—Å–∫–∏–π —Å–ø–µ–ª–ª—á–µ–∫–µ—Ä')
    parser.add_argument('--corpus', default='corpus', 
                       help='–ü—É—Ç—å –∫ –∫–æ—Ä–ø—É—Å—É —Ç–µ–∫—Å—Ç–æ–≤')
    parser.add_argument('--cleaned-corpus', default='cleaned_corpus',
                       help='–ü—É—Ç—å –∫ –æ—á–∏—â–µ–Ω–Ω–æ–º—É –∫–æ—Ä–ø—É—Å—É')
    parser.add_argument('--train', action='store_true', 
                       help='–û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –∫–æ—Ä–ø—É—Å–µ')
    parser.add_argument('--check', type=str, 
                       help='–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–ª–æ–≤–æ –∏–ª–∏ —Ç–µ–∫—Å—Ç')
    parser.add_argument('--model', default='georgian_spellchecker.pkl',
                       help='–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏')
    parser.add_argument('--create-hunspell', action='store_true',
                       help='–°–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª—ã –¥–ª—è Hunspell')
    parser.add_argument('--build', action='store_true',
                       help='–ü–æ–ª–Ω–∞—è —Å–±–æ—Ä–∫–∞ —Å–ø–µ–ª–ª—á–µ–∫–µ—Ä–∞')
    parser.add_argument('--test', action='store_true',
                       help='–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç')
    
    args = parser.parse_args()
    
    if args.build:
        build_complete_spellchecker()
        return
    
    if args.test:
        quick_test()
        return
    
    spell_checker = GeorgianSpellChecker()
    
    if args.train:
        print("=== –û–ë–£–ß–ï–ù–ò–ï –°–ü–ï–õ–õ–ß–ï–ö–ï–†–ê ===")
        
        # –ü—ã—Ç–∞–µ–º—Å—è —Å–Ω–∞—á–∞–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—á–∏—â–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å
        if Path(args.cleaned_corpus).exists():
            print("–ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å...")
            spell_checker.train_from_cleaned_corpus(args.cleaned_corpus)
        else:
            print("–û—á–∏—â–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π...")
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ—Ä–ø—É—Å
            CorpusProcessor.process_existing_corpus(args.corpus, "processed_corpus")
            spell_checker.load_corpus(args.corpus)
        
        # –°—Ç—Ä–æ–∏–º N-gram –º–æ–¥–µ–ª—å
        spell_checker.build_ngram_model(2)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        spell_checker.save_model(args.model)
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª—ã –¥–ª—è Hunspell –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if args.create_hunspell:
            create_hunspell_files(spell_checker.vocabulary, "hunspell_output")
    
    elif args.check:
        print("=== –ü–†–û–í–ï–†–ö–ê –¢–ï–ö–°–¢–ê ===")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        if Path(args.model).exists():
            spell_checker.load_model(args.model)
        else:
            print("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å: --train")
            return
        
        text = args.check
        if len(text.split()) == 1:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞
            word = text.strip()
            if spell_checker.is_correct(word):
                print(f"‚úì –°–ª–æ–≤–æ '{word}' –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ")
            else:
                suggestions = spell_checker.suggest_corrections(word)
                print(f"‚úó –°–ª–æ–≤–æ '{word}' —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—à–∏–±–∫–∏")
                print(f"–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {', '.join(suggestions)}")
        else:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞
            errors = spell_checker.check_text(text)
            if not errors:
                print("‚úì –¢–µ–∫—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—à–∏–±–æ–∫")
            else:
                print(f"‚úó –ù–∞–π–¥–µ–Ω–æ {len(errors)} –æ—à–∏–±–æ–∫:")
                for word, suggestions in errors:
                    print(f"  '{word}' -> {', '.join(suggestions[:3])}")
    
    else:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
        print("  –ü–æ–ª–Ω–∞—è —Å–±–æ—Ä–∫–∞: python georgian_spellchecker.py --build")
        print("  –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å: python georgian_spellchecker.py --train")
        print("  –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–ª–æ–≤–æ: python georgian_spellchecker.py --check '—Å–ª–æ–≤–æ'")
        print("  –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–µ–∫—Å—Ç: python georgian_spellchecker.py --check '–≤–µ—Å—å —Ç–µ–∫—Å—Ç'")
        print("  –°–æ–∑–¥–∞—Ç—å Hunspell: python georgian_spellchecker.py --train --create-hunspell")
        print("  –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç: python georgian_spellchecker.py --test")
        print("  –î–µ–º–æ: python georgian_spellchecker.py")

if __name__ == "__main__":
    # –ï—Å–ª–∏ –∑–∞–ø—É—Å–∫ –±–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ - –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è
    import sys
    if len(sys.argv) == 1:
        demo()
    else:
        main()

def build_complete_model(self, corpus_path: str = None, output_path: str = "georgian_spellchecker.pkl"):
    """–ü–æ–ª–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫–æ—Ä–ø—É—Å–∞"""
    print("üèóÔ∏è –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    
    if corpus_path:
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–æ—Ä–ø—É—Å
        self.load_corpus(corpus_path)
    else:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å –µ—Å–ª–∏ –µ—Å—Ç—å
        cleaned_path = "cleaned_corpus"
        if Path(cleaned_path).exists():
            self.train_from_cleaned_corpus(cleaned_path)
        else:
            # –ò—â–µ–º –∫–æ—Ä–ø—É—Å –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
            possible_corpus_paths = [
                "1_collect/corpus",
                "../1_collect/corpus", 
                "corpus"
            ]
            
            for path in possible_corpus_paths:
                if Path(path).exists():
                    print(f"üìö –ù–∞–π–¥–µ–Ω –∫–æ—Ä–ø—É—Å: {path}")
                    self.load_corpus(path)
                    break
            else:
                print("‚ö†Ô∏è  –ö–æ—Ä–ø—É—Å –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å...")
                # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å
                basic_words = {
                    '·Éí·Éê·Éõ·Éê·É†·ÉØ·Éù·Éë·Éê', '·É†·Éù·Éí·Éù·É†', '·ÉÆ·Éê·É†', '·Éì·É¶·Éî·É°', '·Éô·Éê·É†·Éí·Éò', '·Éê·Éõ·Éò·Éú·Éì·Éò', 
                    '·É°·Éê·É•·Éê·É†·Éó·Éï·Éî·Éö·Éù', '·Éó·Éë·Éò·Éö·Éò·É°·Éò', '·Éî·Éú·Éê', '·Éû·É†·Éù·Éí·É†·Éê·Éõ·Éê', '·Éô·Éù·Éõ·Éû·Éò·É£·É¢·Éî·É†·Éò'
                }
                self.vocabulary = basic_words
                self.word_freq = {word: 1 for word in basic_words}
    
    # –°—Ç—Ä–æ–∏–º N-gram –º–æ–¥–µ–ª–∏
    self.build_ngram_model(2)
    self.build_ngram_model(3)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    self.save_model(output_path)
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")
    print(f"üìä –°–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ: {len(self.vocabulary)}")