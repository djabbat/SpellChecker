# web_interface.py
import os
import sys
import pickle
import re
from pathlib import Path
from collections import Counter, defaultdict
from flask import Flask, request, jsonify, render_template

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¿ÑƒÑ‚ĞµĞ¹
current_dir = Path(__file__).parent
project_root = current_dir.parent

# ĞŸÑƒÑ‚Ğ¸ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ
possible_paths = [
    current_dir,
    project_root / "2_basis",
    project_root / "4_advanced", 
    project_root,
    Path("."),
    Path(".."),
    Path("../2_basis"),
    Path("../4_advanced")
]

for path in possible_paths:
    if path.exists():
        sys.path.insert(0, str(path))

app = Flask(__name__, template_folder='templates', static_folder='static')
app.config['SECRET_KEY'] = 'georgian-spellchecker-secret-key'

# Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ ĞºĞ»Ğ°ÑÑÑ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹
class OptimizedSpellChecker:
    def __init__(self):
        self.vocabulary = set()
        self.word_freq = Counter()
        self._cached_distances = {}
        
    def tokenize_georgian(self, text: str):
        """Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€ÑƒĞ·Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°"""
        words = re.findall(r'[\u10A0-\u10FF]{2,}', text)
        return words
    
    def is_correct(self, word: str):
        return word in self.vocabulary
    
    def optimized_levenshtein(self, s1: str, s2: str):
        """ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ›ĞµĞ²ĞµĞ½ÑˆÑ‚ĞµĞ¹Ğ½Ğ° Ñ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼"""
        cache_key = (s1, s2)
        if cache_key in self._cached_distances:
            return self._cached_distances[cache_key]
            
        if s1 == s2:
            self._cached_distances[cache_key] = 0
            return 0
            
        len1, len2 = len(s1), len(s2)
        if abs(len1 - len2) > 2:
            self._cached_distances[cache_key] = 3
            return 3
            
        if len1 < len2:
            return self.optimized_levenshtein(s2, s1)
            
        if len2 == 0:
            return len1
            
        previous_row = list(range(len2 + 1))
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
            
        result = previous_row[-1]
        self._cached_distances[cache_key] = result
        return result
    
    def generate_candidates_fast(self, word: str, max_distance: int = 1):
        """Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸"""
        if self.is_correct(word):
            return [word]
        
        candidates = []
        word_len = len(word)
        
        for candidate in self.vocabulary:
            if abs(len(candidate) - word_len) > 2:
                continue
                
            if word_len > 2 and candidate[:2] != word[:2]:
                continue
                
            distance = self.optimized_levenshtein(word, candidate)
            if distance <= max_distance:
                candidates.append((candidate, distance))
                
                if len(candidates) >= 20:
                    break
        
        candidates.sort(key=lambda x: (x[1], -self.word_freq.get(x[0], 0)))
        return [candidate for candidate, distance in candidates[:5]]
    
    def suggest_corrections(self, word: str, max_suggestions: int = 3):
        candidates = self.generate_candidates_fast(word)
        return candidates[:max_suggestions]
    
    def check_text_fast(self, text: str, max_errors: int = 50):
        """Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº"""
        words = self.tokenize_georgian(text)
        errors = []
        
        for word in words:
            if not self.is_correct(word):
                suggestions = self.suggest_corrections(word)
                errors.append({
                    'word': word,
                    'suggestions': suggestions,
                    'start_pos': text.find(word),
                    'end_pos': text.find(word) + len(word)
                })
                
                if len(errors) >= max_errors:
                    break
        
        return errors

def load_vocabulary_from_file(file_path):
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ¸Ğ· Ñ„Ğ°Ğ¹Ğ»Ğ°"""
    vocabulary = set()
    word_freq = {}
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    if '\t' in line:
                        parts = line.split('\t')
                        word = parts[0].strip()
                        if len(parts) > 1:
                            try:
                                freq = int(parts[1])
                                word_freq[word] = freq
                            except:
                                word_freq[word] = 1
                    else:
                        word = line.strip()
                        word_freq[word] = 1
                    
                    if word and any('\u10A0' <= char <= '\u10FF' for char in word):
                        vocabulary.add(word)
        
        print(f"   ğŸ“– Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {len(vocabulary)} ÑĞ»Ğ¾Ğ² Ğ¸Ğ· {file_path.name}")
        return vocabulary, word_freq
        
    except Exception as e:
        print(f"   âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ {file_path}: {e}")
        return set(), {}

def initialize_spellcheckers():
    """Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¿ĞµĞ»Ğ»Ñ‡ĞµĞºĞµÑ€Ğ¾Ğ² Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼"""
    global checker, model_info
    
    print("ğŸ” áƒ˜áƒœáƒ˜áƒªáƒ˜áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ áƒ¡áƒáƒ”áƒšáƒ©áƒ”áƒ™áƒ”áƒ áƒ˜áƒ¡...")
    
    checker = OptimizedSpellChecker()
    
    # ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼ĞµÑÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ
    vocabulary_sources = [
        project_root / "2_basis" / "processed_corpus" / "vocabulary.txt",
        project_root / "2_basis" / "hunspell_georgian" / "ka_GE.dic",
        project_root / "2_basis" / "georgian_spellchecker.pkl",
        project_root / "4_advanced" / "advanced_georgian_spellchecker.pkl",
    ]
    
    loaded_vocabulary = False
    best_vocabulary = set()
    best_word_freq = {}
    best_source = ""
    
    for source_path in vocabulary_sources:
        if source_path.exists():
            print(f"   ğŸ” áƒ•áƒáƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ— {source_path}...")
            try:
                if source_path.suffix == '.pkl':
                    with open(source_path, 'rb') as f:
                        model_data = pickle.load(f)
                    
                    vocabulary = set()
                    word_freq = {}
                    
                    if 'vocabulary' in model_data:
                        vocabulary = set(model_data['vocabulary'])
                    elif 'word_freq' in model_data:
                        vocabulary = set(model_data['word_freq'].keys())
                    
                    if 'word_freq' in model_data:
                        word_freq = model_data['word_freq']
                    else:
                        word_freq = {word: 1 for word in vocabulary}
                        
                elif source_path.suffix == '.txt' or source_path.name == 'ka_GE.dic':
                    vocabulary, word_freq = load_vocabulary_from_file(source_path)
                else:
                    continue
                
                if vocabulary and len(vocabulary) > len(best_vocabulary):
                    best_vocabulary = vocabulary
                    best_word_freq = word_freq
                    best_source = source_path.name
                    print(f"   âœ… áƒ•áƒ˜áƒáƒáƒ•áƒ”áƒ— áƒšáƒ”áƒ¥áƒ¡áƒ˜áƒ™áƒáƒœáƒ˜ {len(vocabulary)} áƒ¡áƒ˜áƒ¢áƒ§áƒ•áƒ˜áƒ—")
                    
            except Exception as e:
                print(f"   âŒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ áƒ¤áƒáƒ˜áƒšáƒ˜áƒ¡ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ˜áƒ¡áƒáƒ¡ {source_path}: {e}")
                continue
    
    if best_vocabulary:
        checker.vocabulary = best_vocabulary
        checker.word_freq = Counter(best_word_freq)
        model_info = {
            "type": "production", 
            "vocabulary_size": len(best_vocabulary),
            "source": best_source
        }
        print(f"âœ… áƒšáƒ”áƒ¥áƒ¡áƒ˜áƒ™áƒáƒœáƒ˜ áƒ©áƒáƒ˜áƒ¢áƒ•áƒ˜áƒ áƒ—áƒ {best_source}-áƒ“áƒáƒœ")
        print(f"ğŸ“Š áƒ¡áƒ˜áƒ¢áƒ§áƒ•áƒ”áƒ‘áƒ˜ áƒšáƒ”áƒ¥áƒ¡áƒ˜áƒ™áƒáƒœáƒ¨áƒ˜: {len(best_vocabulary)}")
        loaded_vocabulary = True
    else:
        print("âš ï¸  áƒ áƒ”áƒáƒšáƒ£áƒ áƒ˜ áƒšáƒ”áƒ¥áƒ¡áƒ˜áƒ™áƒáƒœáƒ”áƒ‘áƒ˜ áƒ•áƒ”áƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ. áƒ•áƒ˜áƒ§áƒ”áƒœáƒ”áƒ‘áƒ— áƒ«áƒ˜áƒ áƒ˜áƒ—áƒáƒ“ áƒ¢áƒ”áƒ¡áƒ¢áƒ£áƒ  áƒšáƒ”áƒ¥áƒ¡áƒ˜áƒ™áƒáƒœáƒ¡.")
        test_vocabulary = {
            'áƒ’áƒáƒ›áƒáƒ áƒ¯áƒáƒ‘áƒ', 'áƒ áƒáƒ’áƒáƒ ', 'áƒ®áƒáƒ ', 'áƒ“áƒ¦áƒ”áƒ¡', 'áƒ™áƒáƒ áƒ’áƒ˜', 'áƒáƒ›áƒ˜áƒœáƒ“áƒ˜', 
            'áƒ¡áƒáƒ¥áƒáƒ áƒ—áƒ•áƒ”áƒšáƒ', 'áƒ—áƒ‘áƒ˜áƒšáƒ˜áƒ¡áƒ˜', 'áƒ”áƒœáƒ', 'áƒáƒ áƒáƒ’áƒ áƒáƒ›áƒ', 'áƒ™áƒáƒ›áƒáƒ˜áƒ£áƒ¢áƒ”áƒ áƒ˜',
            'áƒ«áƒáƒšáƒ˜áƒáƒœ', 'áƒšáƒáƒ›áƒáƒ–áƒ˜', 'áƒ¥áƒáƒšáƒáƒ¥áƒ˜', 'áƒ¢áƒ£áƒ áƒ˜áƒ¡áƒ¢áƒ˜', 'áƒ¬áƒ”áƒ áƒ¡', 'áƒ™áƒáƒ“áƒ˜',
            'áƒáƒ˜áƒ—áƒáƒœáƒ˜', 'áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜', 'áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ', 'áƒ¡áƒ¬áƒáƒ áƒ˜', 'áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ', 'áƒ‘áƒ’áƒ”áƒ áƒ'
        }
        
        checker.vocabulary = test_vocabulary
        checker.word_freq = {word: 1 for word in test_vocabulary}
        model_info = {
            "type": "test", 
            "vocabulary_size": len(test_vocabulary),
            "source": "basic_test"
        }
        print(f"âœ… áƒ¨áƒ”áƒ˜áƒ¥áƒ›áƒœáƒ áƒ«áƒ˜áƒ áƒ˜áƒ—áƒáƒ“áƒ˜ áƒ¢áƒ”áƒ¡áƒ¢áƒ£áƒ áƒ˜ áƒšáƒ”áƒ¥áƒ¡áƒ˜áƒ™áƒáƒœáƒ˜ {len(test_vocabulary)} áƒ¡áƒ˜áƒ¢áƒ§áƒ•áƒ˜áƒ—")
        loaded_vocabulary = True
    
    return loaded_vocabulary

# Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ñ€Ñ‚Ğµ
initialize_spellcheckers()

@app.route('/')
def index():
    """Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°"""
    return render_template('index.html', model_info=model_info)

@app.route('/check', methods=['POST'])
def check_text():
    """API Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ°"""
    if not checker:
        return jsonify({'error': 'áƒ¡áƒáƒ”áƒšáƒ©áƒ”áƒ™áƒ”áƒ áƒ˜ áƒáƒ  áƒ˜áƒœáƒ˜áƒªáƒ˜áƒáƒšáƒ˜áƒ–áƒ˜áƒ áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ'}), 500
    
    data = request.get_json()
    text = data.get('text', '')
    
    if not text:
        return jsonify({'errors': [], 'stats': {'total_words': 0, 'error_count': 0}})
    
    try:
        errors = checker.check_text_fast(text, max_errors=100)
        
        return jsonify({
            'errors': errors,
            'stats': {
                'total_words': len(checker.tokenize_georgian(text)),
                'error_count': len(errors)
            }
        })
        
    except Exception as e:
        print(f"âŒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ˜áƒ¡áƒáƒ¡: {e}")
        return jsonify({'error': f'áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ˜áƒ¡áƒáƒ¡: {str(e)}'}), 500

@app.route('/stats')
def get_stats():
    """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"""
    return jsonify(model_info)

@app.route('/health')
def health_check():
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸"""
    return jsonify({'status': 'healthy'})

if __name__ == '__main__':
    print("=" * 60)
    print("ğŸ‡¬ğŸ‡ª áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒ˜ áƒ¡áƒáƒ”áƒšáƒ©áƒ”áƒ™áƒ”áƒ áƒ˜ - áƒ•áƒ”áƒ‘ áƒ˜áƒœáƒ¢áƒ”áƒ áƒ¤áƒ”áƒ˜áƒ¡áƒ˜")
    print("=" * 60)
    print(f"ğŸ“ áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ¢áƒ˜áƒáƒ˜: {model_info['type']}")
    print(f"ğŸ“Š áƒ¡áƒ˜áƒ¢áƒ§áƒ•áƒ”áƒ‘áƒ˜ áƒšáƒ”áƒ¥áƒ¡áƒ˜áƒ™áƒáƒœáƒ¨áƒ˜: {model_info['vocabulary_size']}")
    if model_info.get('source'):
        print(f"ğŸ“ áƒ¬áƒ§áƒáƒ áƒ: {model_info['source']}")
    print(f"ğŸŒ áƒ¡áƒ”áƒ áƒ•áƒ”áƒ áƒ˜ áƒ’áƒáƒ¨áƒ•áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ: http://localhost:5000")
    print("=" * 60)
    print("âœ¨ áƒ’áƒáƒ®áƒ¡áƒ”áƒœáƒ˜áƒ— áƒ‘áƒ áƒáƒ£áƒ–áƒ”áƒ áƒ˜ áƒ“áƒ áƒ’áƒáƒ“áƒáƒ“áƒ˜áƒ— áƒ›áƒ˜áƒ¡áƒáƒ›áƒáƒ áƒ—áƒ–áƒ” áƒ–áƒ”áƒ›áƒáƒ—!")
    print("=" * 60)
    
    app.run(debug=True, host='0.0.0.0', port=5000, threaded=True)