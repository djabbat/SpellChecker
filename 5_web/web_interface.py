# web_interface.py
import os
import sys
import pickle
import re
from pathlib import Path
from collections import Counter, defaultdict
from flask import Flask, request, jsonify, render_template

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¿ÑƒÑ‚ĞµĞ¹
current_dir = Path(__file__).parent
project_root = current_dir.parent

# ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°
possible_paths = [
    current_dir,
    project_root / "2_basis",
    project_root / "4_advanced", 
    project_root,
    Path("."),
    Path(".."),
    Path("../2_basis"),
    Path("../4_advanced"),
    Path("../../2_basis"),  # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸
    Path("../../4_advanced")
]

for path in possible_paths:
    if path.exists():
        sys.path.insert(0, str(path))
        print(f"âœ… Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½ Ğ¿ÑƒÑ‚ÑŒ: {path}")

app = Flask(__name__, template_folder='templates', static_folder='static')
app.config['SECRET_KEY'] = 'georgian-spellchecker-secret-key'

# Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
checker = None
model_info = {}

# Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ ĞºĞ»Ğ°ÑÑÑ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹
class OptimizedSpellChecker:
    def __init__(self):
        self.vocabulary = set()
        self.word_freq = Counter()
        self._cached_distances = {}
        
    def tokenize_georgian(self, text: str):
        """Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€ÑƒĞ·Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°"""
        words = re.findall(r'[\u10A0-\u10FF]{2,}', text)
        return words
    
    def is_correct(self, word: str):
        return word in self.vocabulary
    
    def optimized_levenshtein(self, s1: str, s2: str):
        """ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ›ĞµĞ²ĞµĞ½ÑˆÑ‚ĞµĞ¹Ğ½Ğ° Ñ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼"""
        cache_key = (s1, s2)
        if cache_key in self._cached_distances:
            return self._cached_distances[cache_key]
            
        if s1 == s2:
            self._cached_distances[cache_key] = 0
            return 0
            
        len1, len2 = len(s1), len(s2)
        if abs(len1 - len2) > 2:
            self._cached_distances[cache_key] = 3
            return 3
            
        if len1 < len2:
            return self.optimized_levenshtein(s2, s1)
            
        if len2 == 0:
            return len1
            
        previous_row = list(range(len2 + 1))
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
            
        result = previous_row[-1]
        self._cached_distances[cache_key] = result
        return result
    
    def generate_candidates_fast(self, word: str, max_distance: int = 1):
        """Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸"""
        if self.is_correct(word):
            return [word]
        
        candidates = []
        word_len = len(word)
        
        for candidate in self.vocabulary:
            if abs(len(candidate) - word_len) > 2:
                continue
                
            if word_len > 2 and candidate[:2] != word[:2]:
                continue
                
            distance = self.optimized_levenshtein(word, candidate)
            if distance <= max_distance:
                candidates.append((candidate, distance))
                
                if len(candidates) >= 20:
                    break
        
        candidates.sort(key=lambda x: (x[1], -self.word_freq.get(x[0], 0)))
        return [candidate for candidate, distance in candidates[:5]]
    
    def suggest_corrections(self, word: str, max_suggestions: int = 3):
        candidates = self.generate_candidates_fast(word)
        return candidates[:max_suggestions]
    
    def check_text_fast(self, text: str, max_errors: int = 50):
        """Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº"""
        words = self.tokenize_georgian(text)
        errors = []
        
        for word in words:
            if not self.is_correct(word):
                suggestions = self.suggest_corrections(word)
                start_pos = text.find(word)
                errors.append({
                    'word': word,
                    'suggestions': suggestions,
                    'start_pos': start_pos,
                    'end_pos': start_pos + len(word)
                })
                
                if len(errors) >= max_errors:
                    break
        
        return errors

def load_vocabulary_from_file(file_path):
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ¸Ğ· Ñ„Ğ°Ğ¹Ğ»Ğ°"""
    vocabulary = set()
    word_freq = {}
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    if '\t' in line:
                        parts = line.split('\t')
                        word = parts[0].strip()
                        if len(parts) > 1:
                            try:
                                freq = int(parts[1])
                                word_freq[word] = freq
                            except:
                                word_freq[word] = 1
                    else:
                        word = line.strip()
                        word_freq[word] = 1
                    
                    if word and any('\u10A0' <= char <= '\u10FF' for char in word):
                        vocabulary.add(word)
        
        print(f"   ğŸ“– Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {len(vocabulary)} ÑĞ»Ğ¾Ğ² Ğ¸Ğ· {file_path.name}")
        return vocabulary, word_freq
        
    except Exception as e:
        print(f"   âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ {file_path}: {e}")
        return set(), {}

def load_pickle_model(file_path):
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ· pickle Ñ„Ğ°Ğ¹Ğ»Ğ°"""
    try:
        with open(file_path, 'rb') as f:
            model_data = pickle.load(f)
        
        vocabulary = set()
        word_freq = {}
        
        if 'vocabulary' in model_data:
            vocabulary = set(model_data['vocabulary'])
        elif 'word_freq' in model_data:
            vocabulary = set(model_data['word_freq'].keys())
        
        if 'word_freq' in model_data:
            word_freq = model_data['word_freq']
        else:
            word_freq = {word: 1 for word in vocabulary}
            
        return vocabulary, word_freq, True
        
    except Exception as e:
        print(f"   âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ pickle {file_path}: {e}")
        return set(), {}, False

def initialize_spellcheckers():
    """Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¿ĞµĞ»Ğ»Ñ‡ĞµĞºĞµÑ€Ğ¾Ğ² Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼"""
    global checker, model_info
    
    print("ğŸ” áƒ˜áƒœáƒ˜áƒªáƒ˜áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ áƒ¡áƒáƒ”áƒšáƒ©áƒ”áƒ™áƒ”áƒ áƒ˜áƒ¡...")
    
    checker = OptimizedSpellChecker()
    
    # ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼ĞµÑÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°
    vocabulary_sources = [
        # ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸
        project_root / "2_basis" / "processed_corpus" / "vocabulary.txt",
        project_root / "2_basis" / "hunspell_georgian" / "ka_GE.dic",
        project_root / "2_basis" / "georgian_spellchecker.pkl",
        project_root / "4_advanced" / "advanced_georgian_spellchecker.pkl",
        project_root / "4_advanced" / "merged_georgian_spellchecker.pkl",
        
        # ĞĞ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸
        current_dir / "merged_georgian_spellchecker.pkl",
        current_dir / "fallback_spellchecker.pkl",
        Path("2_basis") / "georgian_spellchecker.pkl",
        Path("4_advanced") / "advanced_georgian_spellchecker.pkl",
    ]
    
    best_vocabulary = set()
    best_word_freq = {}
    best_source = ""
    best_size = 0
    
    for source_path in vocabulary_sources:
        if source_path.exists():
            print(f"   ğŸ” áƒ•áƒáƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ— {source_path}...")
            try:
                if source_path.suffix == '.pkl':
                    vocabulary, word_freq, success = load_pickle_model(source_path)
                    if success and vocabulary:
                        current_size = len(vocabulary)
                        if current_size > best_size:
                            best_vocabulary = vocabulary
                            best_word_freq = word_freq
                            best_source = source_path.name
                            best_size = current_size
                            print(f"   âœ… áƒ•áƒ˜áƒáƒáƒ•áƒ”áƒ— áƒšáƒ”áƒ¥áƒ¡áƒ˜áƒ™áƒáƒœáƒ˜ {current_size} áƒ¡áƒ˜áƒ¢áƒ§áƒ•áƒ˜áƒ—")
                            
                elif source_path.suffix == '.txt' or source_path.name == 'ka_GE.dic':
                    vocabulary, word_freq = load_vocabulary_from_file(source_path)
                    if vocabulary:
                        current_size = len(vocabulary)
                        if current_size > best_size:
                            best_vocabulary = vocabulary
                            best_word_freq = word_freq
                            best_source = source_path.name
                            best_size = current_size
                            print(f"   âœ… áƒ•áƒ˜áƒáƒáƒ•áƒ”áƒ— áƒšáƒ”áƒ¥áƒ¡áƒ˜áƒ™áƒáƒœáƒ˜ {current_size} áƒ¡áƒ˜áƒ¢áƒ§áƒ•áƒ˜áƒ—")
                    
            except Exception as e:
                print(f"   âŒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ áƒ¤áƒáƒ˜áƒšáƒ˜áƒ¡ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ˜áƒ¡áƒáƒ¡ {source_path}: {e}")
                continue
    
    if best_vocabulary:
        checker.vocabulary = best_vocabulary
        checker.word_freq = Counter(best_word_freq)
        model_info = {
            "type": "production", 
            "vocabulary_size": len(best_vocabulary),
            "source": best_source,
            "status": "loaded"
        }
        print(f"âœ… áƒšáƒ”áƒ¥áƒ¡áƒ˜áƒ™áƒáƒœáƒ˜ áƒ©áƒáƒ˜áƒ¢áƒ•áƒ˜áƒ áƒ—áƒ {best_source}-áƒ“áƒáƒœ")
        print(f"ğŸ“Š áƒ¡áƒ˜áƒ¢áƒ§áƒ•áƒ”áƒ‘áƒ˜ áƒšáƒ”áƒ¥áƒ¡áƒ˜áƒ™áƒáƒœáƒ¨áƒ˜: {len(best_vocabulary)}")
        return True
    else:
        print("âš ï¸  áƒ áƒ”áƒáƒšáƒ£áƒ áƒ˜ áƒšáƒ”áƒ¥áƒ¡áƒ˜áƒ™áƒáƒœáƒ”áƒ‘áƒ˜ áƒ•áƒ”áƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ. áƒ•áƒ˜áƒ§áƒ”áƒœáƒ”áƒ‘áƒ— áƒ«áƒ˜áƒ áƒ˜áƒ—áƒáƒ“ áƒ¢áƒ”áƒ¡áƒ¢áƒ£áƒ  áƒšáƒ”áƒ¥áƒ¡áƒ˜áƒ™áƒáƒœáƒ¡.")
        test_vocabulary = {
            'áƒ’áƒáƒ›áƒáƒ áƒ¯áƒáƒ‘áƒ', 'áƒ áƒáƒ’áƒáƒ ', 'áƒ®áƒáƒ ', 'áƒ“áƒ¦áƒ”áƒ¡', 'áƒ™áƒáƒ áƒ’áƒ˜', 'áƒáƒ›áƒ˜áƒœáƒ“áƒ˜', 
            'áƒ¡áƒáƒ¥áƒáƒ áƒ—áƒ•áƒ”áƒšáƒ', 'áƒ—áƒ‘áƒ˜áƒšáƒ˜áƒ¡áƒ˜', 'áƒ”áƒœáƒ', 'áƒáƒ áƒáƒ’áƒ áƒáƒ›áƒ', 'áƒ™áƒáƒ›áƒáƒ˜áƒ£áƒ¢áƒ”áƒ áƒ˜',
            'áƒ«áƒáƒšáƒ˜áƒáƒœ', 'áƒšáƒáƒ›áƒáƒ–áƒ˜', 'áƒ¥áƒáƒšáƒáƒ¥áƒ˜', 'áƒ¢áƒ£áƒ áƒ˜áƒ¡áƒ¢áƒ˜', 'áƒ¬áƒ”áƒ áƒ¡', 'áƒ™áƒáƒ“áƒ˜',
            'áƒáƒ˜áƒ—áƒáƒœáƒ˜', 'áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜', 'áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ', 'áƒ¡áƒ¬áƒáƒ áƒ˜', 'áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ', 'áƒ‘áƒ’áƒ”áƒ áƒ',
            'áƒ¡áƒáƒšáƒáƒ›áƒ˜', 'áƒ‘áƒáƒ áƒ˜', 'áƒ°áƒ”áƒ˜', 'áƒ›áƒáƒ¨áƒ˜áƒœ', 'áƒ¨áƒ”áƒ›áƒ“áƒ”áƒ’', 'áƒáƒ“áƒ áƒ”', 'áƒ’áƒ•áƒ˜áƒáƒœ',
            'áƒ“áƒ˜áƒ“áƒ˜', 'áƒáƒáƒ¢áƒáƒ áƒ', 'áƒáƒ®áƒáƒšáƒ˜', 'áƒ«áƒ•áƒ”áƒšáƒ˜', 'áƒ¡áƒ¬áƒ áƒáƒ¤áƒ˜', 'áƒœáƒ”áƒšáƒ˜', 'áƒªáƒ®áƒ”áƒšáƒ˜',
            'áƒªáƒ˜áƒ•áƒ˜', 'áƒ—áƒ”áƒ—áƒ áƒ˜', 'áƒ¨áƒáƒ•áƒ˜', 'áƒ¬áƒ˜áƒ—áƒ”áƒšáƒ˜', 'áƒ›áƒ¬áƒ•áƒáƒœáƒ”', 'áƒšáƒ£áƒ áƒ¯áƒ˜', 'áƒ§áƒ•áƒ˜áƒ—áƒ”áƒšáƒ˜'
        }
        
        checker.vocabulary = test_vocabulary
        checker.word_freq = {word: 1 for word in test_vocabulary}
        model_info = {
            "type": "test", 
            "vocabulary_size": len(test_vocabulary),
            "source": "basic_test",
            "status": "fallback"
        }
        print(f"âœ… áƒ¨áƒ”áƒ˜áƒ¥áƒ›áƒœáƒ áƒ«áƒ˜áƒ áƒ˜áƒ—áƒáƒ“áƒ˜ áƒ¢áƒ”áƒ¡áƒ¢áƒ£áƒ áƒ˜ áƒšáƒ”áƒ¥áƒ¡áƒ˜áƒ™áƒáƒœáƒ˜ {len(test_vocabulary)} áƒ¡áƒ˜áƒ¢áƒ§áƒ•áƒ˜áƒ—")
        return True

# Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ñ€Ñ‚Ğµ
if not initialize_spellcheckers():
    print("âŒ ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¿ĞµĞ»Ğ»Ñ‡ĞµĞºĞµÑ€Ğ°!")

@app.route('/')
def index():
    """Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°"""
    return render_template('index.html', model_info=model_info)

@app.route('/check', methods=['POST'])
def check_text():
    """API Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ°"""
    if not checker:
        return jsonify({'error': 'áƒ¡áƒáƒ”áƒšáƒ©áƒ”áƒ™áƒ”áƒ áƒ˜ áƒáƒ  áƒ˜áƒœáƒ˜áƒªáƒ˜áƒáƒšáƒ˜áƒ–áƒ˜áƒ áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ'}), 500
    
    data = request.get_json()
    if not data:
        return jsonify({'error': 'áƒáƒ áƒáƒ¡áƒ¬áƒáƒ áƒ˜ áƒ›áƒáƒ—áƒ®áƒáƒ•áƒœáƒ'}), 400
        
    text = data.get('text', '')
    
    if not text:
        return jsonify({'errors': [], 'stats': {'total_words': 0, 'error_count': 0}})
    
    try:
        errors = checker.check_text_fast(text, max_errors=100)
        
        return jsonify({
            'errors': errors,
            'stats': {
                'total_words': len(checker.tokenize_georgian(text)),
                'error_count': len(errors)
            },
            'model_info': model_info
        })
        
    except Exception as e:
        print(f"âŒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ˜áƒ¡áƒáƒ¡: {e}")
        return jsonify({'error': f'áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ˜áƒ¡áƒáƒ¡: {str(e)}'}), 500

@app.route('/suggest/<word>')
def suggest_word(word):
    """API Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°"""
    if not checker:
        return jsonify({'error': 'áƒ¡áƒáƒ”áƒšáƒ©áƒ”áƒ™áƒ”áƒ áƒ˜ áƒáƒ  áƒ˜áƒœáƒ˜áƒªáƒ˜áƒáƒšáƒ˜áƒ–áƒ˜áƒ áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ'}), 500
    
    try:
        suggestions = checker.suggest_corrections(word, max_suggestions=5)
        return jsonify({
            'word': word,
            'is_correct': checker.is_correct(word),
            'suggestions': suggestions
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/stats')
def get_stats():
    """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"""
    return jsonify(model_info)

@app.route('/health')
def health_check():
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸"""
    status = "healthy" if checker else "unhealthy"
    return jsonify({
        'status': status,
        'model_loaded': checker is not None,
        'vocabulary_size': len(checker.vocabulary) if checker else 0
    })

@app.route('/reload', methods=['POST'])
def reload_model():
    """ĞŸĞµÑ€ĞµĞ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"""
    global checker, model_info
    try:
        success = initialize_spellcheckers()
        if success:
            return jsonify({'status': 'success', 'message': 'áƒ›áƒáƒ“áƒ”áƒšáƒ˜ áƒ’áƒáƒœáƒáƒ®áƒšáƒ”áƒ‘áƒ£áƒšáƒ˜áƒ', 'model_info': model_info})
        else:
            return jsonify({'status': 'error', 'message': 'áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ’áƒáƒœáƒáƒ®áƒšáƒ”áƒ‘áƒ áƒ•áƒ”áƒ  áƒ›áƒáƒ®áƒ”áƒ áƒ®áƒ“áƒ'}), 500
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500

if __name__ == '__main__':
    print("=" * 60)
    print("ğŸ‡¬ğŸ‡ª áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒ˜ áƒ¡áƒáƒ”áƒšáƒ©áƒ”áƒ™áƒ”áƒ áƒ˜ - áƒ•áƒ”áƒ‘ áƒ˜áƒœáƒ¢áƒ”áƒ áƒ¤áƒ”áƒ˜áƒ¡áƒ˜")
    print("=" * 60)
    print(f"ğŸ“ áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ¢áƒ˜áƒáƒ˜: {model_info.get('type', 'unknown')}")
    print(f"ğŸ“Š áƒ¡áƒ˜áƒ¢áƒ§áƒ•áƒ”áƒ‘áƒ˜ áƒšáƒ”áƒ¥áƒ¡áƒ˜áƒ™áƒáƒœáƒ¨áƒ˜: {model_info.get('vocabulary_size', 0)}")
    if model_info.get('source'):
        print(f"ğŸ“ áƒ¬áƒ§áƒáƒ áƒ: {model_info['source']}")
    print(f"ğŸŒ áƒ¡áƒ”áƒ áƒ•áƒ”áƒ áƒ˜ áƒ’áƒáƒ¨áƒ•áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ: http://localhost:5000")
    print("=" * 60)
    print("âœ¨ áƒ’áƒáƒ®áƒ¡áƒ”áƒœáƒ˜áƒ— áƒ‘áƒ áƒáƒ£áƒ–áƒ”áƒ áƒ˜ áƒ“áƒ áƒ’áƒáƒ“áƒáƒ“áƒ˜áƒ— áƒ›áƒ˜áƒ¡áƒáƒ›áƒáƒ áƒ—áƒ–áƒ” áƒ–áƒ”áƒ›áƒáƒ—!")
    print("=" * 60)
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ¿Ğ°Ğ¿ĞºĞ¸ ĞµÑĞ»Ğ¸ Ğ¸Ñ… Ğ½ĞµÑ‚
    (current_dir / "templates").mkdir(exist_ok=True)
    (current_dir / "static" / "css").mkdir(parents=True, exist_ok=True)
    (current_dir / "static" / "js").mkdir(parents=True, exist_ok=True)
    
    app.run(debug=True, host='0.0.0.0', port=5000, threaded=True)