#!/usr/bin/env python3
"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –≥—Ä—É–∑–∏–Ω—Å–∫–∏–π —Å–ø–µ–ª–ª—á–µ–∫–µ—Ä —Å N-gram –º–æ–¥–µ–ª—è–º–∏
"""

import pickle
from collections import defaultdict, Counter
from pathlib import Path
import re
from typing import List, Tuple, Set

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –∏–∑ —Ç–æ–≥–æ –∂–µ —Ñ–∞–π–ª–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –µ–≥–æ
class GeorgianSpellChecker:
    def __init__(self):
        self.vocabulary = set()
        self.word_freq = Counter()
        self.ngram_models = {}
        
    def load_corpus(self, corpus_path: str) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ—Ä–ø—É—Å–∞ –∏–∑ –ø–∞–ø–∫–∏"""
        corpus_dir = Path(corpus_path)
        
        if not corpus_dir.exists():
            parent_corpus = Path("..") / corpus_path
            if parent_corpus.exists():
                corpus_dir = parent_corpus
                print(f"–ö–æ—Ä–ø—É—Å –Ω–∞–π–¥–µ–Ω –≤: {parent_corpus}")
        
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ—Ä–ø—É—Å–∞...")
        total_files = 0
        
        for file_path in corpus_dir.glob("**/*.txt"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                words = self.tokenize_georgian(content)
                if words:
                    self.vocabulary.update(words)
                    self.word_freq.update(words)
                
                total_files += 1
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_path}: {e}")
        
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –§–∞–π–ª–æ–≤: {total_files}, –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(self.vocabulary)}")
    
    def tokenize_georgian(self, text: str) -> List[str]:
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≥—Ä—É–∑–∏–Ω—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        text = re.sub(r'[^\u10A0-\u10FF\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        words = text.strip().split()
        return [word for word in words if len(word) > 1]
    
    def is_correct(self, word: str) -> bool:
        return word in self.vocabulary
    
    def generate_candidates(self, word: str, max_distance: int = 2) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è"""
        def levenshtein_distance(s1: str, s2: str) -> int:
            if len(s1) < len(s2):
                return levenshtein_distance(s2, s1)
            if len(s2) == 0:
                return len(s1)
            previous_row = list(range(len(s2) + 1))
            for i, c1 in enumerate(s1):
                current_row = [i + 1]
                for j, c2 in enumerate(s2):
                    insertions = previous_row[j + 1] + 1
                    deletions = current_row[j] + 1
                    substitutions = previous_row[j] + (c1 != c2)
                    current_row.append(min(insertions, deletions, substitutions))
                previous_row = current_row
            return previous_row[-1]
        
        candidates = []
        if self.is_correct(word):
            return [word]
        
        for candidate in self.vocabulary:
            distance = levenshtein_distance(word, candidate)
            if distance <= max_distance:
                candidates.append((candidate, distance))
        
        candidates.sort(key=lambda x: (x[1], -self.word_freq.get(x[0], 0)))
        return [candidate for candidate, distance in candidates[:10]]
    
    def suggest_corrections(self, word: str, max_suggestions: int = 5) -> List[str]:
        candidates = self.generate_candidates(word)
        return candidates[:max_suggestions]
    
    def save_model(self, model_path: str) -> None:
        model_data = {
            'vocabulary': list(self.vocabulary),
            'word_freq': dict(self.word_freq),
            'ngram_models': self.ngram_models
        }
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
    
    def load_model(self, model_path: str) -> None:
        with open(model_path, 'rb') as f:
            model_data = pickle.load(f)
        self.vocabulary = set(model_data['vocabulary'])
        self.word_freq = Counter(model_data['word_freq'])
        self.ngram_models = model_data['ngram_models']
        print(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(self.vocabulary)}")

class AdvancedGeorgianSpellChecker(GeorgianSpellChecker):
    def __init__(self):
        super().__init__()
        self.bigram_model = defaultdict(Counter)
        self.trigram_model = defaultdict(Counter)
        self.context_window = 3
    
    def build_advanced_ngram_models(self, corpus_path: str):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö N-gram –º–æ–¥–µ–ª–µ–π —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
        print("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö N-gram –º–æ–¥–µ–ª–µ–π...")
        
        corpus_dir = Path(corpus_path)
        all_sentences = []
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ –∫–æ—Ä–ø—É—Å–∞
        for file_path in corpus_dir.glob("**/*.txt"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–ø—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥)
                sentences = re.split(r'[.!?]', content)
                for sentence in sentences:
                    words = self.tokenize_georgian(sentence)
                    if len(words) >= 2:  # –¢–æ–ª—å–∫–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å 2+ —Å–ª–æ–≤–∞–º–∏
                        all_sentences.append(words)
                        
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_path}: {e}")
        
        # –°—Ç—Ä–æ–∏–º N-gram –º–æ–¥–µ–ª–∏
        bigram_count = 0
        trigram_count = 0
        
        for sentence in all_sentences:
            # –ë–∏–≥—Ä–∞–º–º—ã
            for i in range(len(sentence) - 1):
                self.bigram_model[sentence[i]][sentence[i + 1]] += 1
                bigram_count += 1
            
            # –¢—Ä–∏–≥—Ä–∞–º–º—ã
            for i in range(len(sentence) - 2):
                key = (sentence[i], sentence[i + 1])
                self.trigram_model[key][sentence[i + 2]] += 1
                trigram_count += 1
        
        print(f"N-gram –º–æ–¥–µ–ª–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã! –ë–∏–≥—Ä–∞–º–º: {bigram_count}, –¢—Ä–∏–≥—Ä–∞–º–º: {trigram_count}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±–∏–≥—Ä–∞–º–º: {len(self.bigram_model)}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç—Ä–∏–≥—Ä–∞–º–º: {len(self.trigram_model)}")
    
    def suggest_with_context(self, word: str, previous_words: List[str] = None, max_suggestions: int = 5) -> List[str]:
        """–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        candidates = self.generate_candidates(word)
        
        if not candidates or not previous_words:
            return candidates[:max_suggestions]
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        scored_candidates = []
        
        for candidate in candidates:
            score = 0
            
            # –£—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–µ–µ —Å–ª–æ–≤–æ (–±–∏–≥—Ä–∞–º–º–∞)
            if len(previous_words) >= 1:
                prev_word = previous_words[-1]
                if prev_word in self.bigram_model:
                    score += self.bigram_model[prev_word].get(candidate, 0) * 2
            
            # –£—á–∏—Ç—ã–≤–∞–µ–º –¥–≤–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–ª–æ–≤–∞ (—Ç—Ä–∏–≥—Ä–∞–º–º–∞)
            if len(previous_words) >= 2:
                prev_key = (previous_words[-2], previous_words[-1])
                if prev_key in self.trigram_model:
                    score += self.trigram_model[prev_key].get(candidate, 0) * 3
            
            # –£—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å —Å–ª–æ–≤–∞
            score += self.word_freq.get(candidate, 0) * 0.1
            
            scored_candidates.append((candidate, score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score
        scored_candidates.sort(key=lambda x: x[1], reverse=True)
        
        return [candidate for candidate, score in scored_candidates[:max_suggestions]]
    
    def check_text_with_context(self, text: str) -> List[Tuple[str, List[str], List[str]]]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        words = self.tokenize_georgian(text)
        errors = []
        
        for i, word in enumerate(words):
            if not self.is_correct(word):
                # –ë–µ—Ä–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                context_start = max(0, i - self.context_window)
                previous_words = words[context_start:i]
                
                suggestions = self.suggest_with_context(word, previous_words)
                errors.append((word, suggestions, previous_words))
        
        return errors
    
    def save_advanced_model(self, model_path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –º–æ–¥–µ–ª–∏"""
        model_data = {
            'vocabulary': list(self.vocabulary),
            'word_freq': dict(self.word_freq),
            'bigram_model': dict(self.bigram_model),
            'trigram_model': {str(k): v for k, v in self.trigram_model.items()},
            'ngram_models': self.ngram_models
        }
        
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
    
    def load_advanced_model(self, model_path: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –º–æ–¥–µ–ª–∏"""
        with open(model_path, 'rb') as f:
            model_data = pickle.load(f)
        
        self.vocabulary = set(model_data['vocabulary'])
        self.word_freq = Counter(model_data['word_freq'])
        self.bigram_model = defaultdict(Counter, model_data['bigram_model'])
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç—Ä–∏–≥—Ä–∞–º–º—ã
        self.trigram_model = defaultdict(Counter)
        for k, v in model_data['trigram_model'].items():
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∫–ª—é—á –æ–±—Ä–∞—Ç–Ω–æ –≤ tuple
            if k.startswith('(') and k.endswith(')'):
                key_tuple = tuple(k[1:-1].replace("'", "").split(', '))
                self.trigram_model[key_tuple] = Counter(v)
        
        self.ngram_models = model_data['ngram_models']
        print(f"–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –°–ª–æ–≤: {len(self.vocabulary)}")

def test_advanced_spellchecker():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ —Å–ø–µ–ª–ª—á–µ–∫–µ—Ä–∞"""
    print("=== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–†–û–î–í–ò–ù–£–¢–û–ì–û –°–ü–ï–õ–õ–ß–ï–ö–ï–†–ê ===")
    
    checker = AdvancedGeorgianSpellChecker()
    
    # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å
    model_path = "georgian_spellchecker.pkl"
    advanced_model_path = "advanced_georgian_spellchecker.pkl"
    
    if Path(advanced_model_path).exists():
        print("–ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –º–æ–¥–µ–ª—å...")
        checker.load_advanced_model(advanced_model_path)
    elif Path(model_path).exists():
        print("–ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å...")
        checker.load_model(model_path)
        
        # –°—Ç—Ä–æ–∏–º N-gram –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è
        print("–°—Ç—Ä–æ–∏–º N-gram –º–æ–¥–µ–ª–∏...")
        corpus_path = "../1.Collect a text corpus/corpus"
        if Path(corpus_path).exists():
            checker.build_advanced_ngram_models(corpus_path)
            checker.save_advanced_model(advanced_model_path)
        else:
            print("–ö–æ—Ä–ø—É—Å –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è N-gram –º–æ–¥–µ–ª–µ–π")
    else:
        print("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å.")
        return
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
    test_cases = [
        "·Éí·Éê·Éõ·Éê·É†·ÉØ·Éù·Éë·Éê ·É†·Éù·Éí·Éù·É† ·ÉÆ·Éê·É† ·Éì·É¶·Éî·É°",
        "·Éí·Éê·Éõ·Éê·É†·ÉØ·Éê·Éë·Éê ·É†·Éù·Éí·Éù·Éó ·ÉÆ·Éê·É†·Éó ·Éô·Éê·É†·Éí·Éê·Éì", 
        "·Éî·É° ·Éê·É†·Éò·É° ·É°·Éê·É¢·Éî·É°·É¢·Éù ·É¢·Éî·É•·É°·É¢·Éò ·É®·Éî·É™·Éì·Éù·Éõ·Éî·Éë·Éò·Éó",
        "·Éû·É†·Éù·Éí·É†·Éê·Éõ·Éê ·Éô·Éù·Éõ·Éû·Éò·É£·É¢·Éî·É†·Éò ·É¢·Éî·É•·Éú·Éù·Éö·Éù·Éí·Éò·Éê"
    ]
    
    for text in test_cases:
        print(f"\n–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞: '{text}'")
        errors = checker.check_text_with_context(text)
        
        if errors:
            for error_word, suggestions, context in errors:
                print(f"  –û—à–∏–±–∫–∞: '{error_word}'")
                print(f"  –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}")
                print(f"  –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {suggestions[:3]}")
        else:
            print("  ‚úì –û—à–∏–±–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
    print("\n=== –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ö–û–ù–¢–ï–ö–°–¢–ù–û–ô –ü–†–û–í–ï–†–ö–ò ===")
    demo_words = [
        ("·Éí·Éê·Éõ·Éê·É†·ÉØ·Éê·Éë·Éê", ["·Éì·Éò·Éö·Éê"]),  # "·Éí·Éê·Éõ·Éê·É†·ÉØ·Éê·Éë·Éê" –ø–æ—Å–ª–µ "·Éì·Éò·Éö·Éê"
        ("·É†·Éù·Éí·Éù·Éó", ["·Éí·Éê·Éõ·Éê·É†·ÉØ·Éù·Éë·Éê"]),  # "·É†·Éù·Éí·Éù·Éó" –ø–æ—Å–ª–µ "·Éí·Éê·Éõ·Éê·É†·ÉØ·Éù·Éë–∞"
    ]
    
    for word, context in demo_words:
        suggestions = checker.suggest_with_context(word, context)
        print(f"–°–ª–æ–≤–æ '{word}' –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ {context} -> {suggestions[:3]}")

def build_advanced_spellchecker():
    """–ü–æ–ª–Ω–∞—è —Å–±–æ—Ä–∫–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ —Å–ø–µ–ª–ª—á–µ–∫–µ—Ä–∞"""
    print("=== –ü–û–°–¢–†–û–ï–ù–ò–ï –ü–†–û–î–í–ò–ù–£–¢–û–ì–û –°–ü–ï–õ–õ–ß–ï–ö–ï–†–ê ===")
    
    checker = AdvancedGeorgianSpellChecker()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
    base_model_path = "../2.Cleaning and normalization/georgian_spellchecker.pkl"
    if Path(base_model_path).exists():
        print("–ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å...")
        checker.load_model(base_model_path)
    else:
        print("–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é...")
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Å–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        return
    
    # –°—Ç—Ä–æ–∏–º N-gram –º–æ–¥–µ–ª–∏
    corpus_path = "../1.Collect a text corpus/corpus"
    if Path(corpus_path).exists():
        checker.build_advanced_ngram_models(corpus_path)
    else:
        print("–ö–æ—Ä–ø—É—Å –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –º–æ–¥–µ–ª—å
    advanced_model_path = "advanced_georgian_spellchecker.pkl"
    checker.save_advanced_model(advanced_model_path)
    
    print("\n=== –ü–†–û–î–í–ò–ù–£–¢–´–ô –°–ü–ï–õ–õ–ß–ï–ö–ï–† –ü–û–°–¢–†–û–ï–ù ===")
    print(f"–ú–æ–¥–µ–ª—å: {advanced_model_path}")
    print(f"–°–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ: {len(checker.vocabulary)}")
    print(f"–ë–∏–≥—Ä–∞–º–º: {sum(len(v) for v in checker.bigram_model.values())}")
    print(f"–¢—Ä–∏–≥—Ä–∞–º–º: {sum(len(v) for v in checker.trigram_model.values())}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--build":
        build_advanced_spellchecker()
    else:
        test_advanced_spellchecker()
def build_complete_advanced_model(self, corpus_path: str = None, output_path: str = "advanced_georgian_spellchecker.pkl"):
    """–ü–æ–ª–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –º–æ–¥–µ–ª–∏"""
    print("üß† –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –º–æ–¥–µ–ª–∏...")
    
    # –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –µ—Å—Ç—å
    basic_model_path = "../2_basis/georgian_spellchecker.pkl"
    if Path(basic_model_path).exists():
        print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
        self.load_model(basic_model_path)
    elif corpus_path:
        # –°—Ç—Ä–æ–∏–º —Å –Ω—É–ª—è –∏–∑ –∫–æ—Ä–ø—É—Å–∞
        print("üìö –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ—Ä–ø—É—Å–∞...")
        self.load_corpus(corpus_path)
    else:
        print("‚ö†Ô∏è  –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—â–µ–º –∫–æ—Ä–ø—É—Å...")
        # –ü–æ–∏—Å–∫ –∫–æ—Ä–ø—É—Å–∞
        possible_corpus_paths = [
            "../1_collect/corpus",
            "1_collect/corpus",
            "corpus"
        ]
        
        for path in possible_corpus_paths:
            if Path(path).exists():
                print(f"üìö –ù–∞–π–¥–µ–Ω –∫–æ—Ä–ø—É—Å: {path}")
                self.load_corpus(path)
                break
        else:
            print("‚ùå –ö–æ—Ä–ø—É—Å –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            return False
    
    # –°—Ç—Ä–æ–∏–º N-gram –º–æ–¥–µ–ª–∏
    print("üî® –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ N-gram –º–æ–¥–µ–ª–µ–π...")
    if corpus_path:
        self.build_advanced_ngram_models(corpus_path)
    else:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ—Ä–ø—É—Å –∏–∑ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        corpus_path = "../1_collect/corpus"
        if Path(corpus_path).exists():
            self.build_advanced_ngram_models(corpus_path)
        else:
            print("‚ö†Ô∏è  –ö–æ—Ä–ø—É—Å –¥–ª—è N-gram –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –º–æ–¥–µ–ª—å
    self.save_advanced_model(output_path)
    print(f"‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")
    print(f"üìä –°–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ: {len(self.vocabulary)}")
    if hasattr(self, 'bigram_model'):
        print(f"üìà –ë–∏–≥—Ä–∞–º–º: {sum(len(v) for v in self.bigram_model.values())}")
    if hasattr(self, 'trigram_model'):
        print(f"üìà –¢—Ä–∏–≥—Ä–∞–º–º: {sum(len(v) for v in self.trigram_model.values())}")
    
    return True